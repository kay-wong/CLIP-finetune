{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qovET6WkSMBk",
        "outputId": "4ae7790e-ace0-4567-ea15-653123856463"
      },
      "outputs": [],
      "source": [
        "# !pip install -q -U jax jaxlib\n",
        "!pip install -q -U flax\n",
        "!pip install -q sentence-transformers\n",
        "#!pip install -q git+https://github.com/huggingface/transformers.git\n",
        "!pip install -q transformers\n",
        "!pip install -q torch torchvision\n",
        "!pip install -q comet_ml==3.12.2\n",
        "#!pip install -r drive/MyDrive/ColabNotebooks/hybrid_clip/requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NspXz7MRjg-j",
        "outputId": "6f6fae50-2056-4753-bdf5-7ffed634d0cf"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qScLOJOaNIGD"
      },
      "outputs": [],
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "# notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yq6ktq1K0YsG"
      },
      "outputs": [],
      "source": [
        "import os \n",
        "import sys\n",
        "import json\n",
        "import zipfile\n",
        "import natsort\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import urllib.request\n",
        "\n",
        "from IPython.display import display, Image\n",
        "\n",
        "from PIL import Image as PilImage\n",
        "\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = \"false\"\n",
        "\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import CenterCrop, ConvertImageDtype, Normalize, Resize, ToTensor\n",
        "from torchvision.transforms.functional import InterpolationMode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ4Ffu7459Xr"
      },
      "outputs": [],
      "source": [
        "#from modeling_hybrid_clip import FlaxHybridCLIP\n",
        "from transformers import FlaxVisionTextDualEncoderModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mD75f5xrg4GI"
      },
      "outputs": [],
      "source": [
        "PROJ_DIR= \"drive/MyDrive/ColabNotebooks/\"\n",
        "TRN_IMG_ZIP_FOLDER = PROJ_DIR+ \"data/CLIP/train_v2/trn_img_tags_v2_100k_p2.zip\"\n",
        "VALID_IMG_ZIP_FOLDER = PROJ_DIR+ \"data/CLIP/eval_v2/eval_img_tags_v2_10k.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ws2JejKh3uW_"
      },
      "outputs": [],
      "source": [
        "SCRIPT_DIR = \"drive/MyDrive/ColabNotebooks/hybrid_clip\"\n",
        "MODEL_DIR = \"drive/MyDrive/ColabNotebooks/models/clip-finetuned\"\n",
        "IMAGE_ENCODER=\"openai/clip-vit-base-patch32\"\n",
        "TEXT_ENCODER=\"cardiffnlp/twitter-roberta-base-jun2022\"\n",
        "\n",
        "TRAIN_FILE= PROJ_DIR+\"data/CLIP/train_v2/trn_img_tags_v2_100k_p2.json\"\n",
        "VALID_FILE= PROJ_DIR+\"data/CLIP/eval_v2/valid_img_tags_v2_10k.json\"\n",
        "IMG_FOLDER = './images'\n",
        "VALID_IMG_FOLDER = './valid_images'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twCZJJCgzopH"
      },
      "source": [
        "### Setup images train and valid data folder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bnZUdp5Z93CK",
        "outputId": "f4c3a8cb-27f0-4b12-a06f-f65d70ef35bb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting: 100%|██████████| 99079/99079 [01:50<00:00, 893.01it/s]\n"
          ]
        }
      ],
      "source": [
        "## Unzip images folder\n",
        "if not os.path.exists(IMG_FOLDER) or len(os.listdir(IMG_FOLDER)) == 0:\n",
        "    os.makedirs(IMG_FOLDER, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(TRN_IMG_ZIP_FOLDER, 'r') as zf:\n",
        "    for member in tqdm(zf.infolist(), desc='Extracting'):\n",
        "        zf.extract(member, IMG_FOLDER)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "34qQIUBdWxE7",
        "outputId": "995f4f76-54ec-4588-9e96-9cba14c1db3b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Extracting: 100%|██████████| 11896/11896 [00:24<00:00, 489.83it/s]\n"
          ]
        }
      ],
      "source": [
        "## Unzip valid images\n",
        "if not os.path.exists(VALID_IMG_FOLDER) or len(os.listdir(VALID_IMG_FOLDER)) == 0:\n",
        "    os.makedirs(VALID_IMG_FOLDER, exist_ok=True)\n",
        "with zipfile.ZipFile(VALID_IMG_ZIP_FOLDER, 'r') as zf:\n",
        "    for member in tqdm(zf.infolist(), desc='Extracting'):\n",
        "        zf.extract(member, VALID_IMG_FOLDER)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yX5Ra5dizy5j"
      },
      "source": [
        "### Call train script"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "LDcWOXN4C5lz",
        "outputId": "7429d3b4-cf66-4872-d957-216b3694a0e6"
      },
      "outputs": [],
      "source": [
        "!export COMET_API_KEY=\"\"\n",
        "!python -W ignore $SCRIPT_DIR/run_hybrid_clip.py \\\n",
        "    --output_dir $MODEL_DIR \\\n",
        "    --overwrite_output_dir \\\n",
        "    --tokenizer_name=$TEXT_ENCODER \\\n",
        "    --train_file=$TRAIN_FILE \\\n",
        "    --validation_file=$VALID_FILE \\\n",
        "    --do_train --do_eval \\\n",
        "    --num_train_epochs=\"100\" --max_seq_length 24 \\\n",
        "    --per_device_train_batch_size=\"128\" \\\n",
        "    --per_device_eval_batch_size=\"128\" \\\n",
        "    --learning_rate=\"0.0001\" --warmup_ratio 0.1 --weight_decay 0.0 \\\n",
        "    --preprocessing_num_workers 8 \\\n",
        "    --exp_name clip_finetune_v2 \\\n",
        "    --eval_when 1 \\\n",
        "    --text_model_name_or_path=$TEXT_ENCODER \\\n",
        "    --vision_model_name_or_path=$IMAGE_ENCODER \\\n",
        "    --log_comet \\\n",
        "    --epoch_offset 44 \\\n",
        "    --run_from_checkpoint $FROM_CHECKPOINT\n",
        "#    --freeze_backbones \\ #Try unfreeze backbones"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
